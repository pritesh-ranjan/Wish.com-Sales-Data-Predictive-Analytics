{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EDA of Sales of summer clothes in E-commerce Wish dataset\n\nFirst of all we will build a function to analyse the sales data for us and provide important information such as:\n* Data type of each field\n* Which columns has missing data and number of missing records in each column\n* What is the correlation of all the other numeric columns with the target column \n* Now for the numeric features: the mean, median and mode\n* For categorical columns: mode\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"product_sales_df = pd.read_csv('/kaggle/input/summer-products-and-sales-in-ecommerce-wish/summer-products-with-rating-and-performance_2020-08.csv')\ncat_count_df = pd.read_csv('/kaggle/input/summer-products-and-sales-in-ecommerce-wish/unique-categories.sorted-by-count.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_col = 'units_sold'\n\nprint(f\"Shape of dataframe {product_sales_df.shape}\")\n\nrows = []\nfor col in product_sales_df.columns:\n    if product_sales_df[col].isin([0, 1, np.nan]).all():\n        row_dict = {'ColumnName': col, 'DataType': 'binary', 'HasMissing':product_sales_df.isnull().any().loc[col],\n                    'NumberOfMissingCells': product_sales_df.isnull().sum().loc[col], 'CorrelationWithTarget': product_sales_df.corr()[target_col].loc[col],\n                    'Mean': np.nan, 'Median': np.nan,'Mode': product_sales_df.mode()[col].loc[0], 'MinValue': np.nan, 'MaxValue': np.nan }\n    \n    elif product_sales_df.dtypes.loc[col] == 'int64' or product_sales_df.dtypes.loc[col] == 'float64':\n        \n        row_dict = {'ColumnName': col, 'DataType': product_sales_df.dtypes.loc[col], 'HasMissing':product_sales_df.isnull().any().loc[col],\n                    'NumberOfMissingCells': product_sales_df.isnull().sum().loc[col], 'CorrelationWithTarget': product_sales_df.corr()[target_col].loc[col],\n                    'Mean': product_sales_df.mean().loc[col], 'Median':product_sales_df.median().loc[col], 'Mode': product_sales_df.mode()[col].loc[0],\n                    'MinValue': product_sales_df.min().loc[col], 'MaxValue': product_sales_df.max().loc[col] }\n        \n    else:\n        row_dict = {'ColumnName': col, 'DataType': product_sales_df.dtypes.loc[col], 'HasMissing':product_sales_df.isnull().any().loc[col],\n                    'NumberOfMissingCells': product_sales_df.isnull().sum().loc[col], 'CorrelationWithTarget': np.nan, 'Mean': np.nan, 'Median': np.nan,\n                    'Mode': product_sales_df.mode()[col].loc[0], 'MinValue': np.nan, 'MaxValue': np.nan }\n        \n    rows.append(row_dict)\n    \n        \ninfo_df = pd.DataFrame(rows, columns=['ColumnName', 'DataType', 'HasMissing', 'NumberOfMissingCells', 'CorrelationWithTarget', 'Mean', 'Median', 'Mode', 'MinValue', 'MaxValue'])\n\ninfo_df.set_index('ColumnName', inplace=True)\n    \ninfo_df = info_df.sort_values('CorrelationWithTarget', ascending=False, na_position='last')\n\nprint(\"FOR NUMERICAL COLUMNS\")\ninfo_df[info_df['DataType']!='object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nFor categorical/non-numeric columns\")\ninfo_df[info_df['DataType']=='object'].drop(['CorrelationWithTarget', 'Mean', 'Median', 'MinValue', 'MaxValue'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(product_sales_df.corr()[[target_col]].sort_values(by=target_col, ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title(f'Features Correlating with {target_col}', fontdict={'fontsize':18}, pad=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's first start with categorical features and process them\n\n### 1. currency_buyer\nIt has no missing values and the only value is 'EUR'"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['currency_buyer'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. product_color\nHas 41 missing values and most common value is black"},{"metadata":{"trusted":true},"cell_type":"code","source":"count = product_sales_df['product_color'].value_counts()\ncount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sort(product_sales_df['product_color'].dropna().unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are so many colours, let's see if we can combine different shades of a colour into one colour like : example navy blue, blue and light blue into just blue"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_color'] = product_sales_df['product_color'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df[product_sales_df['product_color'].str.contains('&', na=False)]['product_color'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shade_to_colour = {\n    'navyblue': 'blue', 'lightblue': 'blue', 'skyblue': 'blue', 'lakeblue': 'blue', 'darkblue': 'blue', 'denimblue': 'blue', 'navy blue': 'blue', 'prussianblue': 'blue',\n    'navy': 'blue',\n    'armygreen': 'green', 'army green': 'green', 'fluorescentgreen': 'green', 'mintgreen': 'green', 'light green': 'green', 'lightgreen': 'green',\n    'applegreen': 'green', 'darkgreen': 'green', 'army': 'green', 'khaki': 'green', 'lightkhaki': 'green',\n    'lightyellow': 'yellow', \n    'winered': 'red', 'wine red': 'red', 'lightred': 'red', 'coralred': 'red', 'rose red': 'red', 'watermelonred': 'red', 'orange-red': 'red', 'rosered': 'red',\n    'claret': 'red', 'burgundy': 'red', \n    'gray': 'grey', 'silver': 'grey','lightgray': 'grey', 'lightgrey': 'grey', 'greysnakeskinprint': 'grey',\n    'coffee': 'brown', 'camel': 'brown', 'tan': 'brown', \n    'offwhite': 'white', 'ivory': 'white', 'nude': 'white',\n    'lightpink': 'pink', 'dustypink':'pink', 'rosegold': 'pink',\n    'lightpurple': 'purple', 'coolblack': 'black', 'apricot': 'orange', 'offblack': 'black'\n}\n\ndef update_color(col):\n    if shade_to_colour.get(col, False):\n        return shade_to_colour.get(col)\n    elif '&' in col:\n        return 'dual'\n    elif col in shade_to_colour.values():\n        return col\n    else:\n        return 'other'\n\nproduct_sales_df['product_color'].replace(np.nan, 'others', inplace=True)\n\nproduct_sales_df['product_color'] = product_sales_df.product_color.apply(update_color)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = product_sales_df['product_color'].value_counts()\ncount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_df = product_sales_df.groupby('product_color').agg('sum')['units_sold'].to_frame()\ncol_df.reset_index(level=0, inplace=True)\ncol_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.barplot(x=\"product_color\", y=\"units_sold\", data=col_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After applying the necesssary transformation of the colour column we can see that black has sold most units followe by white."},{"metadata":{},"cell_type":"markdown","source":"## Tags\nNo missing values\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['tags_count'] = product_sales_df['tags'].str.split(',').str.len()\n\nfig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.lineplot(data=product_sales_df, x=\"tags_count\", y=\"units_sold\", ci=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Products with tags more than 35 are more discoverable and are thus bought more often.There is a sudden spike at just below 10 tags so let's investigate if that's an outlier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df[product_sales_df['tags_count']<=10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are only 27 products with tags count less than 10 and only 2 with sales of 20000 and rest have sales like 50, 100, 1000, 5000\nSo these two are outlires and thus reson for spike"},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the most common tags with the help of a wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nustr = \" \".join(product_sales_df['tags'].str.lower().str.split(',').sum())\n\nfig = plt.gcf()\nfig.set_size_inches( 16, 10)\nwordcloud = WordCloud(background_color='white').generate(ustr) \nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. product_variation_size_id\n14 missing values\nmost common value is 'S'"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_variation_size_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_variation_size_id'].value_counts().head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets try to reduce the number of sizes here. "},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].str.lower().str.replace('.', '').str.replace('size--', '').str.replace('size -', '').str.replace('size/', '').str.replace('size ', '').str.replace('size-', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_variation_size_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can still few are left."},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('2xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('3xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('4xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('5xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('6xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('x   l', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('sizel', 'l')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('size4xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('x   l', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('1 pc - xl', 'xl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_size(cl):\n    if cl in 'xl,l,s,xs,m,xxl,xxxs,xxxxxl,xxxxl'.split(','):\n        return cl\n    else:\n        return 'other'\n\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace(np.nan, 'OTHER')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].apply(change_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.barplot(x=\"product_variation_size_id\", y=\"units_sold\", data=product_sales_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.countplot('product_variation_size_id',\n              order = product_sales_df['product_variation_size_id'].value_counts().index,\n              data = product_sales_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the majority of the products are of size 'S' but size 'M' has the most units_sold"},{"metadata":{},"cell_type":"markdown","source":"shipping_option_name"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.groupby('shipping_option_name').agg(['count', 'sum'])['units_sold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 25, 16)\nsns.barplot(x=\"shipping_option_name\", y=\"units_sold\", data=product_sales_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## urgency_text"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.urgency_text.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop  urgency_text,\ntitle, title_orig, currency_buyer, urgency_tex, merchant_title,merchant_name, \nmerchant_id,merchant_profile_picture,product_url\tobject, product_picture, \nproduct_id, theme and crawl_month"},{"metadata":{},"cell_type":"markdown","source":"## origin_country"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.groupby('origin_country').agg(['count', 'sum'])['units_sold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_na_merchants = product_sales_df[product_sales_df['origin_country'].isna()]['merchant_id'].values\n\nfor m in list_of_na_merchants:\n    print(\"merchant title \" + m)\n    print(product_sales_df[product_sales_df['merchant_id']==m]['origin_country'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So merchants which have missing origin country cannot be replaced by looking at the origin_country of the same merchant for another product.\nThis is based on the assumnption that a country of origin for a merchant should have the same value across product.\n\nWe will replace this with CN as its the most frequent value.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['origin_country'].fillna('CN', inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are done with processing the categorical columns.\nNow lets start  with the numerical data\n\nThe first column is 'units_sold' which is our target variable. But we don't want to predict the number of sales for a product but rather if the product has been successful on the Wish.com platform.\nSo we will start by converting the numerical data to a binary one. (successful or not)\nunits_sold has no missing values and has a median of 1000. So we will consider products to be successful if they have sales greater than 1000.\nNow we are using median because because it not affected by very high outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['success'] = product_sales_df['units_sold'].apply(lambda x: 1 if x>1000 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}